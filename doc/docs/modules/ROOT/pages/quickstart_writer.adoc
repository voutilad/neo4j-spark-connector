=== Getting Started

The following section will cover the DataSource Writer aspects this means about how to transfer,
the Spark's Dataset content into Neo4j.

Given the following Scala Program:

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

import scala.util.Random

val sparkSession = SparkSession.builder().getOrCreate()
import sparkSession.implicits._

case class Point3d(`type`: String = "point-3d",
                   srid: Int,
                   x: Double,
                   y: Double,
                   z: Double)

case class Person(name: String, surname: String, age: Int, livesIn: Point3d)

val total = 10
val rand = Random
val ds = (1 to total)
  .map(i => Person(name = "Andrea " + i, "Santurbano " + i, rand.nextInt(100),
    Point3d(srid = 4979, x = 12.5811776, y = 41.9579492, z = 1.3))).toDS()

ds.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.ErrorIfExists)
  .option("url", "bolt://localhost:7687")
  .option("labels", ":Person:Customer")
  .save()
----

Will insert 10 nodes into Neo4j via Spark, and each of these will have:
* 2 `labels`: `Person` and `Customer`
* 4 `properties`: `name`, `surname`, `age` and `livesIn`

==== Save Mode

In order to persist data into Neo4j the Spark Connector supports two save mode that will
work only if `UNIQUE` or `NODE KEY` constraints are defined into Neo4j for the given properties:

* `SaveMode.ErrorIfExists`: this will build a `CREATE` query
* `SaveMode.Overwrite`: this will build a `MERGE` query

==== Options

The DataSource Writer has several options in order to connect and persist data into Neo4j.

.Most Common Needed Configuration Settings
|===
|Setting Name |Description |Default Value |Required

|`labels`
|: separated list of the labels to attach to the node.
|_(none)_
|No

|`batch.size`
|The number of the rows sent to Neo4j as batch.
|5000
|No

|`node.keys`
|The comma separated list of properties considered as node keys in case of you're using
`SaveMode.Overwrite`
|_(none)_
|No

|`transaction.codes.fail`
|Comma separated list of Neo4j
|_(none)_
|No

|===

==== How the Spark Connector persist the data

[NOTE]
As the Neo4j Spark Connector provide batch writes in order to speed-up the ingestion process
so if in the process at some point fails all the previous data is already persisted.

===== Nodes

In case you use the option `labels` the Spark Connector will persist the entire Dataset as nodes.
Depending on the `SaveMode` it will `CREATE` or `MERGE` nodes (in the last case using the `node.keys`
properties).
The nodes will be sent to Neo4j in a batch of rows defined in the `batch.size` property and we will
perform the under the hood un `UNWIND` operation over the batch.

I.e. given the following script:

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

import scala.util.Random

val sparkSession = SparkSession.builder().getOrCreate()
import sparkSession.implicits._

case class Point3d(`type`: String = "point-3d",
                   srid: Int,
                   x: Double,
                   y: Double,
                   z: Double)

case class Person(name: String, surname: String, age: Int, livesIn: Point3d)

val total = 10
val rand = Random
val ds = (1 to total)
  .map(i => Person(name = "Andrea " + i, "Santurbano " + i, rand.nextInt(100),
    Point3d(srid = 4979, x = 12.5811776, y = 41.9579492, z = 1.3))).toDS()

ds.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.ErrorIfExists)
  .option("url", "bolt://localhost:7687")
  .option("labels", ":Person:Customer")
  .save()
----

Under the hod the Spark Connector will perform the following Cypher query:

[source,cypher]
----
UNwIND $events AS event
CREATE (n:`Person`:`Customer`) SET n += event.properties
----

For the same script as above except for this part

----
ds.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("url", "bolt://localhost:7687")
  .option("labels", ":Person:Customer")
  .option("node.keys", "name,surname")
  .save()
----

Under the hod the Spark Connector will perform the following Cypher query:

[source,cypher]
----
UNwIND $events AS event
MERGE (n:`Person`:`Customer` {name: event.keys.name, surname: event.keys.surname})
SET n += event.properties
----

In case of the column value is a Map<String, `Value`> (where value can be any supported
https://neo4j.com/docs/cypher-manual/current/syntax/values/[Neo4j Type]) the Connector will automatically
try to flatten it, so if you have the follwing Dataset:

|===
|id |name |lives_in

|1
|Andrea Santurbano
|{address: 'Times Square, 1', city: 'NY', state: 'NY'}

|2
|Davide Fantuzzi
|{address: 'Statue of Liberty, 10', city: 'NY', state: 'NY'}

|===

Under the hod the Spark Connector will flatten the data in this way:

|===
|id |name |`lives_in.address` |`lives_in.address` |`lives_in.city` |`lives_in.state`

|1
|Andrea Santurbano
|Times Square, 1
|NY
|NY

|1
|Davide Fantuzzi
|Statue of Liberty, 10
|NY
|NY

|===

===== Query

In case you use the option `query` the Spark Connector will persist the entire Dataset by using the provided query.
The nodes will be sent to Neo4j in a batch of rows defined in the `batch.size` property and we will
perform the under the hood un `UNWIND` operation over the batch.

So given the following simple Spark program:

----
ds.write
  .format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("query", "CREATE (n:Person{fullName: event.name + event.surname})")
  .save()
----

Under the hod the Spark Connector will perform the following Cypher query:

[source,cypher]
----
UNwIND $events AS event
CREATE (n:Person{fullName: event.name + event.surname})
----

Where `event` represents each Dataset row.

=== Write data by Relationship Type

You can write a dataframe to Neo4j by specifying source, target and relation.
There are two strategies you can use to write relationships: *NATIVE* and *KEYS*

==== Native Strategy

This strategy is useful when you have a schema that conforms with the <<quickstart_reader.adoc#bookmark-rel-read-schema,Relationship Read Schema>>

```scala
df.write
    .format("org.neo4j.spark.DataSource")
    .option("url", "bolt://localhost:7687")
    .option("relationship", "SOLD")
    .option("relationship.source.labels", ":Person")
    .option("relationship.source.save.mode", "Overwrite")
    .option("relationship.target.labels", ":Product")
    .option("relationship.target.save.mode", "Overwrite")
    .save()
```

You just need to specify the source node labels, the target node labels, and the relationship you want between them.

[NOTE]
The default save mode for source and target nodes is `match`. This means that the relationship will be created only if the nodes are already in your DB. Look at <<node-save-modes,here>> for more info about node save modes.

Using `Overwrite` or `Match` node save mode, you can speficy which keys should be used to write the nodes.

```scala
df.write
    .format("org.neo4j.spark.DataSource")
    .option("url", "bolt://localhost:7687")
    .option("relationship", "SOLD")
    .option("relationship.source.labels", ":Person")
    .option("relationship.source.save.mode", "Overwrite")
    .option("relationship.source.node.keys", "id:personId")
    .option("relationship.target.labels", ":Product")
    .option("relationship.target.save.mode", "ErrorIfExists")
    .save()
```

This will use the `personId` column from the DataFrame to match the nodes in the query, producing a statement similar to this:

```
MERGE (source:Person {id: event.personId})
```

Same thing applies for additional properties to be written to the node:

```scala
df.write
    ...
    .option("relationship.source.save.mode", "Overwrite")
    .option("relationship.source.node.properties", "name:firstname,age:age")
    ...
    .save()
```

[[keys-strategy]]
==== Keys Strategy

For all the other cases you can usee the *KEYS* strategy.
Say you have a Dataframe like this:

```scala
val musicDf = Seq(
        (12, "John Bonham", "Drums"),
        (19, "John Mayer", "Guitar"),
        (32, "John Scofield", "Guitar"),
        (15, "John Butler", "Guitar")
    ).toDF("experience", "name", "instrument")
```

To write it to Neo4j is enough to speficy a list of attributes and

```scala
musicDf.write
    .format("org.neo4j.spark.DataSource")
    .option("url", "bolt://localhost:7687")
    .option("relationship", "PLAYS")
    .option("relationship.save.strategy", "keys")
    .option("relationship.source.labels", ":Musician")
    .option("relationship.source.save.mode", "overwrite")
    .option("relationship.source.node.keys", "name:name")
    .option("relationship.target.labels", ":Instrument")
    .option("relationship.target.node.keys", "instrument:name")
    .option("relationship.target.save.mode", "overwrite")
    .save()
```

[[node-keys]]
Here you must speficy which keys of your Dataframe will be written in the source node and in the target node.
All the remaining properties will be written as the relationship properties.
You can do this with a comma-separated list of `key:value` paris, where the key is the node property name to be written, and the value is the dataframe column name.

[[node-save-modes]]
===== Node Save Modes

You can specify 3 different modes to use for saving the nodes:

* `Overwrite`: will perform a `MERGE` on that node
* `ErrorIfExists`: will perform a `CREATE`
* `Match`: will perform a `MATCH`


=== Schema Optimization Operations

The spark connector supports schema optimization operations via:

* index
* constraints
* set of schema queries

that will be executed *before* the import process will start in order to speed-up the import itself.

You can set the optimization via `schema.optimization.type` option that takes three values:

* `INDEX`: it creates only indexes on provided nodes
* `NODE_CONSTRAINTS`: it creates only indexes on provided nodes
* `QUERY`: it perform a series of schema queries separated by `;`

and it works only when you're merging nodes.

==== Index Creation

Following an example of how to create indexes while you're creating nodes

----
ds.write
      .format(classOf[DataSource].getName)
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization.type", "INDEX")
      .save()
----

This will create, before the import starts, the following schema query:

----
CREATE INDEX ON :Person(surname)
----

*So please into consideration that the first label is used for the index creation*


==== Constraint Creation

Following an example of how to create indexes while you're creating nodes

----
ds.write
      .format(classOf[DataSource].getName)
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization.type", "NODE_CONSTRAINTS")
      .save()
----

This will create, before the import starts, the following schema query:

----
CREATE CONSTRAINT ON (p:Person) ASSERT (p.surname) IS UNIQUE
----

*So please into consideration that the first label is used for the index creation*

==== Index/Constraint Creation via schema query

Following an example of how to create indexes while you're creating nodes

----
ds.write
      .format(classOf[DataSource].getName)
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization.type", "QUERY")
      .option("schema.optimization.query", "CREATE INDEX ON :Person(surname); CREATE CONSTRAINT ON (p:Product) ASSERT (p.name, p.sku) IS NODE KEY")
      .save()
----

Before the import starts, the connector will use the schema query that you provided into the
`schema.optimization.query` option.
