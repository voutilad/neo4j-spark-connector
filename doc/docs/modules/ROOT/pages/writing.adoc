= Writing to Neo4j

The following section will cover the DataSource Writer, and how to transfer the Spark Dataset content into Neo4j.

== Getting Started

Given the following Scala Program:

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}
import scala.util.Random

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

case class Point3d(`type`: String = "point-3d",
                   srid: Int,
                   x: Double,
                   y: Double,
                   z: Double)

case class Person(name: String, surname: String, age: Int, livesIn: Point3d)

val total = 10
val rand = Random
val ds = (1 to total)
  .map(i => {
    Person(name = "Andrea " + i, "Santurbano " + i, rand.nextInt(100),
    Point3d(srid = 4979, x = 12.5811776, y = 41.9579492, z = 1.3))
  }).toDS()

ds.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.ErrorIfExists)
  .option("url", "bolt://localhost:7687")
  .option("labels", ":Person:Customer")
  .save()
----

Will insert 10 nodes into Neo4j via Spark, and each of these will have:

* 2 labels: `Person` and `Customer`
* 4 properties: `name`, `surname`, `age` and `livesIn`

[[bookmark-save-mode]]
== Save Mode

In order to persist data into Neo4j the Spark Connector supports two save modes that will
work only if `UNIQUE` or `NODE KEY` constraints are defined into Neo4j for the given properties:

* `SaveMode.ErrorIfExists`: this will build a `CREATE` query
* `SaveMode.Overwrite`: this will build a `MERGE` query

== Options

The DataSource Writer has several options in order to connect and persist data into Neo4j.

.List of available write options
|===
|Setting Name |Description |Default Value |Required

|`labels`
|Colon separated list of the labels to attach to the node
|_(none)_
|No

|`batch.size`
|The number of the rows sent to Neo4j as batch
|5000
|No

|`transaction.codes.fail`
|Comma separated list of Neo4j codes that will cause the transaction to fail
|_(none)_
|No

4+|*Node Specific Options*

|`node.keys`
|Comma separated list of properties considered as node keys in case of you're using
`SaveMode.Overwrite`
|_(none)_
|No

4+|*Relationship Specific Options*

|`relationship.save.strategy`
|<<bookmark-strategies,Save strategy>> to be used.
Default is `native`
|_(empty)_
|Yes

|`relationship.source.labels`
|Colon separated list of labels that identify the *source* node
|_(empty)_
|Yes

|`relationship.source.node.keys`
|Map used as keys for matching the *source* node
|_(empty)_
|No

|`relationship.source.save.mode`
|Source <<bookmark-node-save-modes,node Save Mode>>
|`Match`
|No

|`relationship.source.node.props`
|Map used as keys for specify the *source* properties. Only used if `relationship.save.strategy` is `keys`
|_(empty)_
|No

|`relationship.target.labels`
|Colon separated list of labels that identify the *target* node
|_(empty)_
|Yes

|`relationship.target.node.keys`
|Map used as keys for matching the *target* node
|_(empty)_
|No

|`relationship.target.save.mode`
|Target <<bookmark-node-save-modes,node Save Mode>>
|`Match`
|No

|`relationship.target.node.props`
|Map used as keys for specify the *target* properties. Only used if `relationship.save.strategy` is `keys`
|_(empty)_
|No

|===

[NOTE]
As the Neo4j Spark Connector provide batch writes in order to speed-up the ingestion process
so if in the process at some point fails all the previous data is already persisted.

== Read Data

Writing data to a Neo4j Database can be done in 3 ways:

* <<bookmark-write-query,Custom Cypher Query>>
* <<bookmark-write-node,Node>>
* <<bookmark-write-rel,Relationship>>

[[bookmark-write-query]]
=== Custom Cypher Query

In case you use the option `query` the Spark Connector will persist the entire Dataset by using the provided query.
The nodes will be sent to Neo4j in a batch of rows defined in the `batch.size` property and we will
wrap your query in an `UNWIND $events AS event` statement.

So given the following simple Spark program:

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

val df = (1 to 10)/*...*/.toDF()
df.write
  .format("org.neo4j.spark.DataSource")
  .option("url", "bolt://localhost:7687")
  .option("query", "CREATE (n:Person {fullName: event.name + event.surname})")
  .save()
----

This will be the generated query:

[source,cypher]
----
UNWIND $events AS event
CREATE (n:Person {fullName: event.name + event.surname})
----

Where `events` is the batch creted from your dataset.

[[bookmark-write-node]]
=== Node

In case you use the option `labels` the Spark Connector will persist the entire Dataset as nodes.
Depending on the `SaveMode` it will `CREATE` or `MERGE` nodes (in the last case using the `node.keys`
properties).

The nodes will be sent to Neo4j in a batch of rows defined in the `batch.size` property and we will
perform an UNWIND operation under the hood.

Let's take our first example:

.ErrorIfExists mode
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}
import scala.util.Random

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

case class Point3d(`type`: String = "point-3d",
                   srid: Int,
                   x: Double,
                   y: Double,
                   z: Double)

case class Person(name: String, surname: String, age: Int, livesIn: Point3d)

val total = 10
val rand = Random
val df = (1 to total)
  .map(i => {
    Person(name = "Andrea " + i, "Santurbano " + i, rand.nextInt(100),
    Point3d(srid = 4979, x = 12.5811776, y = 41.9579492, z = 1.3))
  }).toDF()

df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.ErrorIfExists)
  .option("url", "bolt://localhost:7687")
  .option("labels", ":Person:Customer")
  .save()
----

This will be converted in a similar query:

[source,cypher]
----
UNWIND $events AS event
CREATE (n:`Person`:`Customer`) SET n += event.properties
----

If we instead use the same DataFrame but we save it in `Overwrite` mode:

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

val df = (1 to 10)/*...*/.toDF()

df.write
  .format("org.neo4j.spark.DataSource")
  .mode(SaveMode.Overwrite)
  .option("url", "bolt://localhost:7687")
  .option("labels", ":Person:Customer")
  .option("node.keys", "name,surname")
  .save()
----

The generated query will be

[source,cypher]
----
UNNIND $events AS event
MERGE (n:`Person`:`Customer` {name: event.keys.name, surname: event.keys.surname})
SET n += event.properties
----

In case the column value is a Map<String, `Value`> (where `Value` can be any supported
link:https://neo4j.com/docs/cypher-manual/current/syntax/values/[Neo4j Type]) the Connector will
automatically try to flatten it.

Let's say you have the following Dataset:

|===
|id |name |lives_in

|1
|Andrea Santurbano
|{address: 'Times Square, 1', city: 'NY', state: 'NY'}

|2
|Davide Fantuzzi
|{address: 'Statue of Liberty, 10', city: 'NY', state: 'NY'}

|===

Neo4j Spark connector will flatten the maps and each map value will be in it's own property.

|===
|id |name |lives_in.address |lives_in.city |lives_in.state

|1
|Andrea Santurbano
|Times Square, 1
|NY
|NY

|2
|Davide Fantuzzi
|Statue of Liberty, 10
|NY
|NY

|===

[[bookmark-write-rel]]
=== Relationship

You can write a dataframe to Neo4j by specifying source, target and relation.

==== Overview

We need to spend a some words on this method since its a bit complex, and the combinations of options are quite a few.
So we feel the need to clarify the vocabulary first, before diving into the actual process.

Theory is simple, we take your Dataset and we move the columns around to create source and target nodes,
eventually creating the speficied relationship between these two.

This is a basic example of what would happen.
[source,cypher]
----
UNWIND $events AS event
CREATE (source:Person)
SET source = event.source
CREATE (target:Product)
SET target = event.target
CREATE (source)-[rel:BOUGHT]->(target)
SET rel += event.rel
----

The `CREATE` keywords for the source and target nodes can be replaced by a `MERGE` or a `MATCH`.
To control this you can use the <<bookmark-node-save-modes,node save modes>>.
You can set source and target indipendently by using `relationship.source.save.mode` or ``relationship.target.save.mode`.

When using `MATCH` or `MERGE` you will need to specify keys that identify the nodes.
This is what the options `relationship.source.node.keys` and `relationship.target.node.keys`.
More on this <<bookmark-rel-specify-keys,here>>.

The `CREATE` keyword for the relationship can be replaced by a `MERGE`.
You can control this with <<bookmark-save-mode,Save Mode>>.

You are also required to specify one of the two <<bookmark-strategies,Save Strategies>>.
This will identify which method will be used to create the Cypher query
and can have additional options available.

[[bookmark-strategies]]
==== Save Strategies

There are two strategies you can use to write relationships: <<bookmark-strategy-native,Native>> (default strategy) and <<bookmark-strategy-keys,Keys>>.

[[bookmark-strategy-native]]
==== Native Strategy

This strategy is useful when you have a schema that conforms with the <<reading.adoc#bookmark-rel-schema-no-map,Relationship Read Schema>>, with the `relationship.nodes.map` set to false.

Let's say we want to read relationship from a Database, filter them, and write the result to another Database:

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

val originalDf = spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://allprod.host.com:7687")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "false")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()

originalDf
    .where("`target.price` > 2000")
    .write
    .format("org.neo4j.spark.DataSource")
    .option("url", "bolt://expensiveprod.host.com:7687")
    .option("relationship", "SOLD")
    .option("relationship.source.labels", ":Person:Rich")
    .option("relationship.source.save.mode", "ErrorIfExists")
    .option("relationship.target.labels", ":Product:Expensive")
    .option("relationship.target.save.mode", "ErrorIfExists")
    .save()
----

You just need to specify the source node labels, the target node labels, and the relationship you want between them.

The generated query will be:
[source,cypher]
----
UNWIND $events AS event
CREATE (source:Person:Rich)
SET source = event.source
CREATE (target:Product:Expensive)
SET target = event.target
CREATE (source)-[rel:BOUGHT]->(target)
SET rel += event.rel
----

`event.source`, `event.target`, and `event.rel` will contain the column described <<reading.adoc#bookmark-rel-schema-columns,here>>.

[NOTE]
The default save mode for source and target nodes is `Match`.
This means that the relationship will be created only if the nodes are already in your DB.
Look at <<bookmark-node-save-modes,here>> for more info about node save modes.

When using `Overwrite` or `Match` node save mode, you should specify which keys should be used to identify the nodes.

.The Dataframe we are working with
|===
|<rel.id>|<rel.type>|<source.id>|<source.labels>|source.id|source.fullName|<target.id>|<target.labels>|target.name|target.id|rel.quantity

|4|BOUGHT|1|[Person]|1|John Doe|0|[Product]|Product 1|52|240
|5|BOUGHT|3|[Person]|2|Jane Doe|2|[Product]|Product 2|53|145
|===

[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()

// we read our DF from Neo4j using the relationship method
val df = spark.read.format("org.neo4j.spark.DataSource")
  .option("url", "bolt://first.host.com:7687")
  .option("relationship", "BOUGHT")
  .option("relationship.nodes.map", "false")
  .option("relationship.source.labels", "Person")
  .option("relationship.target.labels", "Product")
  .load()

df.write
  .format("org.neo4j.spark.DataSource")
  .option("url", "bolt://second.host.com:7687")
  .option("relationship", "SOLD")
  .option("relationship.source.labels", ":Person:Rich")
  .option("relationship.source.save.mode", "Overwrite")
  .option("relationship.source.node.keys", "source.fullName:fullName")
  .option("relationship.target.labels", ":Product:Expensive")
  .option("relationship.target.save.mode", "Overwrite")
  .option("relationship.target.node.keys", "target.id:id")
  .save()
----

The generated query will be:
[source,cypher]
----
UNWIND $events AS event
MERGE (source:Person:Rich {fullName: event.source.fullName})
SET source = event.source
MERGE (target:Product:Expensive {id: event.target.id})
SET target = event.target
CREATE (source)-[rel:BOUGHT]->(target)
SET rel += event.rel
----

[NOTE]
Remember that you can choose to `CREATE` or `MERGE` the relationship with the <<bookmark-save-mode,save mode>>.

[[bookmark-strategy-keys]]
==== Keys Strategy

When you want more control on the relationship writing you can use the *KEYS* strategy.

As the native strategy, you can specify node keys to identify nodes.
In addition you can also specify which columns should be written as nodes properties.

[[bookmark-rel-specify-keys]]
.Specify keys
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

val musicDf = Seq(
        (12, "John Bonham", "Drums"),
        (19, "John Mayer", "Guitar"),
        (32, "John Scofield", "Guitar"),
        (15, "John Butler", "Guitar")
    ).toDF("experience", "name", "instrument")

musicDf.write
    .format("org.neo4j.spark.DataSource")
    .option("url", "bolt://localhost:7687")
    .option("relationship", "PLAYS")
    .option("relationship.save.strategy", "keys")
    .option("relationship.source.labels", ":Musician")
    .option("relationship.source.save.mode", "overwrite")
    .option("relationship.source.node.keys", "name:name")
    .option("relationship.target.labels", ":Instrument")
    .option("relationship.target.node.keys", "instrument:name")
    .option("relationship.target.save.mode", "overwrite")
    .save()
----

This will create a `MERGE` query using `name` property as key for `Musician` nodes.
The value of `instrument` column will be used as value for `Instrument` property `name`, generating a statement like:
`MERGE (target:Instrument {name: event.target.instrument})`

Here you must specify which keys of your Dataframe will be written in the source node and in the target node.
All the remaining properties will be written as the relationship properties.
You can do this with the options `relationship.source.node.properties` and `relationship.target.node.properties`,
specifying a comma-separated list of `key:value` pairs, where the key is the dataframe column name,
and the value is the node property name.

.Specify properties and keys
[source,scala]
----
import org.apache.spark.sql.{SaveMode, SparkSession}

val spark = SparkSession.builder().getOrCreate()
import spark.implicits._

val musicDf = Seq(
        (12, "John Bonham", "Orange", "Drums"),
        (19, "John Mayer", "White", "Guitar"),
        (32, "John Scofield", "Black", "Guitar"),
        (15, "John Butler", "Wooden", "Guitar")
    ).toDF("experience", "name", "instrument_color", "instrument")

musicDf.write
    .format("org.neo4j.spark.DataSource")
    .option("url", "bolt://localhost:7687")
    .option("relationship", "PLAYS")
    .option("relationship.save.strategy", "keys")
    .option("relationship.source.labels", ":Musician")
    .option("relationship.source.save.mode", "overwrite")
    .option("relationship.source.node.keys", "name:name")
    .option("relationship.target.labels", ":Instrument")
    .option("relationship.target.node.keys", "instrument:name")
    .option("relationship.target.node.properties", "instrument_color:color")
    .option("relationship.target.save.mode", "overwrite")
    .save()
----

[NOTE]
If a column is not mapped as key / property for `source` or `target` nodes will be *added as relationship property*

[[bookmark-node-save-modes]]
===== Node Save Modes

You can specify 3 different modes to use for saving the nodes:

* `Overwrite`: will perform a `MERGE` on that node
* `ErrorIfExists`: will perform a `CREATE`
* `Match`: will perform a `MATCH`


=== Schema Optimization Operations

The spark connector supports schema optimization operations via:

* index
* constraints
* set of schema queries

that will be executed *before* the import process will start in order to speed-up the import itself.

You can set the optimization via `schema.optimization.type` option that takes three values:

* `INDEX`: it creates only indexes on provided nodes
* `NODE_CONSTRAINTS`: it creates only indexes on provided nodes
* `QUERY`: it perform a series of schema queries separated by `;`

and it works only when you're merging nodes.

==== Index Creation

Following an example of how to create indexes while you're creating nodes

----
ds.write
      .format(classOf[DataSource].getName)
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization.type", "INDEX")
      .save()
----

This will create, before the import starts, the following schema query:

----
CREATE INDEX ON :Person(surname)
----

*So please into consideration that the first label is used for the index creation*


==== Constraint Creation

Following an example of how to create indexes while you're creating nodes

----
ds.write
      .format(classOf[DataSource].getName)
      .mode(SaveMode.Overwrite)
      .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
      .option("labels", ":Person:Customer")
      .option("node.keys", "surname")
      .option("schema.optimization.type", "NODE_CONSTRAINTS")
      .save()
----

This will create, before the import starts, the following schema query:

----
CREATE CONSTRAINT ON (p:Person) ASSERT (p.surname) IS UNIQUE
----

*So please into consideration that the first label is used for the index creation*

=== Script Option

The script option allow you to execute a series of preparation script before Spark
Job execution, the result of the last query can be reused in combination with the
`query` ingestion mode as it follows

----
val ds = Seq(SimplePerson("Andrea", "Santurbano")).toDS()

ds.write
  .format(classOf[DataSource].getName)
  .mode(SaveMode.ErrorIfExists)
  .option("url", SparkConnectorScalaSuiteIT.server.getBoltUrl)
  .option("query", "CREATE (n:Person{fullName: event.name + ' ' + event.surname, age: scriptResult[0].age})")
  .option("script",
    """CREATE INDEX ON :Person(surname);
      |CREATE CONSTRAINT ON (p:Product)
      | ASSERT (p.name, p.sku)
      | IS NODE KEY;
      |RETURN 36 AS age;
      |""".stripMargin)
  .save()
----

Before the import starts, the connector will run the content of the `script` option
and the result of the last query will be injected into the `query`; in the end the full
query executed by the connector while is ingesting the data will be

----
WITH $scriptResult AS scriptResult
UNWIND $events AS event
CREATE (n:Person{fullName: event.name + ' ' + event.surname, age: scriptResult[0].age})
----

where `scriptResult` is the result from the last query contained into the `script` options
that is `RETURN 36 AS age;`